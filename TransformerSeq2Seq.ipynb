{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TransformerSeq2Seq_201611283이우람.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j65i49dU8Cjs","executionInfo":{"status":"ok","timestamp":1638349504913,"user_tz":-540,"elapsed":19210,"user":{"displayName":"우람이뭐하니","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcqGQ8DD1PE4ZzNLcy7NNEWePugigQphJ5VzuiNw=s64","userId":"17584337561729816007"}},"outputId":"49b62ba3-4934-4b40-a5af-f4b528588680"},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"0zVeC5Mf8C-S","executionInfo":{"status":"ok","timestamp":1638349525024,"user_tz":-540,"elapsed":5881,"user":{"displayName":"우람이뭐하니","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcqGQ8DD1PE4ZzNLcy7NNEWePugigQphJ5VzuiNw=s64","userId":"17584337561729816007"}}},"source":["from torch.utils.data import (DataLoader, TensorDataset)\n","from torch import nn\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import os\n","\n","class TransformerChat(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        # 전체 단어(음절) 개수\n","        self.vocab_size = config[\"vocab_size\"]\n","\n","        # 단어(음절) 벡터 크기\n","        self.embedding_size = config['embedding_size']\n","\n","        # Transformer의 Attention Head 개수\n","        self.num_heads = config['num_heads']\n","\n","        # Transformer Encoder의 Layer 수\n","        self.num_encoder_layers = config['num_encoder_layers']\n","\n","        # Transformer Decoder의 Layer 수\n","        self.num_decoder_layers = config['num_decoder_layers']\n","\n","        # 입력 Sequence의 최대 길이\n","        self.max_length = config['max_length']\n","\n","        # Transformer 내부 FNN 크기\n","        self.hidden_size = config['hidden_size']\n","\n","        # Token Embedding Matrix 선언\n","        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n","\n","        # Transformer Encoder-Decoder 설계(선언)\n","        self.transformer = nn.Transformer(d_model=self.embedding_size, nhead=self.num_heads, num_encoder_layers=self.num_encoder_layers,\n","                                          num_decoder_layers=self.num_decoder_layers, dim_feedforward=self.hidden_size)\n","       \n","        # 입력 길이 L에 대한 (L X L) mask 생성: 이전 토큰들의 정보만을 반영하기 위한 mask\n","        #       [[1, -inf, -inf, -inf],\n","        #        [1,    1, -inf, -inf],\n","        #               ......\n","        #        [1,    1,    1,    1]]\n","        # 이곳을 채우세요.\n","        self.mask = self.transformer.generate_square_subsequent_mask(self.max_length).cuda()\n","        # 전체 단어 분포로 변환하기 위한 linear\n","        # 이곳을 채우세요.\n","        self.projection_layer = nn.Linear(self.embedding_size, self.vocab_size)\n","\n","    def forward(self, enc_inputs, dec_inputs):\n","\n","        # enc_inputs: [batch, seq_len], dec_inputs: [batch, seq_len]\n","        # enc_input_features: [batch, seq_len, emb_size] -> [seq_len, batch, emb_size]\n","\n","        # enc_input_features  = enc_inputs.unsqueeze(-1).expand(-1, -1, self.embedding_size).transpose(0,1).type(torch.cuda.FloatTensor)\n","        enc_input_features = self.embeddings(enc_inputs).transpose(0,1)\n","        # dec_input_features: [batch, seq_len, emb_size] -> [seq_len, batch, emb_size]\n","        # dec_input_features = dec_inputs.unsqueeze(-1).expand(-1, -1, self.embedding_size).transpose(0,1).type(torch.cuda.FloatTensor)\n","        dec_input_features = self.embeddings(dec_inputs).transpose(0,1)\n","\n","        # dec_output_features: [seq_len, batch, emb_size]\n","        dec_output_features = self.transformer(src=enc_input_features, tgt=dec_input_features, src_mask = self.mask, tgt_mask = self.mask)\n","\n","        # hypothesis : [seq_len, batch, vocab_size]\n","        hypothesis = self.projection_layer(dec_output_features)\n","\n","        return hypothesis"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmYHzEcU8nRp","executionInfo":{"status":"ok","timestamp":1638349525025,"user_tz":-540,"elapsed":7,"user":{"displayName":"우람이뭐하니","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcqGQ8DD1PE4ZzNLcy7NNEWePugigQphJ5VzuiNw=s64","userId":"17584337561729816007"}}},"source":["# 어휘사전(vocabulary) 생성 함수\n","def load_vocab(file_dir):\n","\n","    with open(file_dir,'r',encoding='utf8') as vocab_file:\n","        char2idx = {}\n","        idx2char = {}\n","        index = 0\n","        for char in vocab_file:\n","            char = char.strip()\n","            char2idx[char] = index\n","            idx2char[index] = char\n","            index+=1\n","\n","    return char2idx, idx2char\n","\n","# 문자 입력열을 인덱스로 변환하는 함수\n","def convert_data2feature(config, input_sequence, char2idx, decoder_input=False):\n","\n","    # 고정 길이 벡터 생성\n","    input_features = np.zeros(config[\"max_length\"], dtype=np.int)\n","\n","    if decoder_input:\n","        # Decoder Input은 Target Sequence에서 Right Shift\n","        # Target Sequence :         [\"안\",\"녕\",\"하\",\"세\",\"요\", \"</S>\" ]\n","        # Decoder Input Sequence :  [\"<S>\", \"안\",\"녕\",\"하\",\"세\",\"요\"]\n","        # 이곳을 채우세요.\n","        input_sequence = \" \".join([\"<S>\"] + input_sequence.split()[:-1])\n","\n","    for idx, token in enumerate(input_sequence.split()):\n","        if token in char2idx.keys():\n","            input_features[idx] = char2idx[token]\n","        else:\n","            input_features[idx] = char2idx['<UNK>']\n","\n","    return input_features\n","\n","# 데이터 읽기 함수\n","def load_dataset(config):\n","\n","    # 어휘사전 읽어오기\n","    char2idx, idx2char = load_vocab(config['vocab_file'])\n","\n","    file_dir = config['train_file']\n","    data_file = open(file_dir,'r',encoding='utf8').readlines()\n","\n","    # 데이터를 저장하기 위한 리스트 생성\n","    enc_inputs, dec_inputs, dec_outputs = [], [], []\n","\n","    for line in tqdm(data_file):\n","\n","        line = line.strip().split('\\t')\n","\n","        input_sequence = line[0]\n","        output_sequence = line[1]\n","\n","        enc_inputs.append(convert_data2feature(config, input_sequence, char2idx))\n","        dec_inputs.append(convert_data2feature(config, output_sequence, char2idx, True))\n","        dec_outputs.append(convert_data2feature(config, output_sequence, char2idx))\n","\n","    # 전체 데이터를 저장하고 있는 리스트를 텐서 형태로 변환\n","    enc_inputs = torch.tensor(enc_inputs, dtype=torch.long)\n","    dec_inputs = torch.tensor(dec_inputs, dtype=torch.long)\n","    dec_outputs = torch.tensor(dec_outputs, dtype=torch.long)\n","\n","    return enc_inputs, dec_inputs, dec_outputs, char2idx, idx2char"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"fJ-bIKnw-1D8","executionInfo":{"status":"ok","timestamp":1638349525974,"user_tz":-540,"elapsed":9,"user":{"displayName":"우람이뭐하니","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcqGQ8DD1PE4ZzNLcy7NNEWePugigQphJ5VzuiNw=s64","userId":"17584337561729816007"}}},"source":["# 텐서를 리스트로 변환하는 함수\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()\n","\n","def do_test(config, model, word2idx, idx2word, input_sequence=\"오늘 약속있으세요?\"):\n","\n","    # 평가 모드 셋팅\n","    model.eval()\n","\n","    # 입력된 문자열의 음절을 공백 단위 토큰으로 변환. 공백은 <SP>로 변환: \"오늘 약속\" -> \"오 늘 <SP> 약 속\"\n","    input_sequence = \" \".join([e if e != \" \" else \"<SP>\" for e in input_sequence])\n","\n","    # 텐서 변환: [1, seq_len]\n","    enc_inputs = torch.tensor([convert_data2feature(config, input_sequence, word2idx)], dtype=torch.long).cuda()\n","    \n","    # input_ids : [1, seq_len] -> 첫번째 디코더 입력 \"<S>\" 만들기\n","    dec_inputs = torch.tensor([convert_data2feature(config, \"\", word2idx, True)], dtype=torch.long).cuda()\n","    \n","    # 시스템 응답 문자열 초기화\n","    response = ''\n","\n","    # 최대 입력 길이 만큼 Decoding Loop\n","    for decoding_step in range(config['max_length']-1):\n","\n","        # dec_outputs: [vocab_size]\n","        dec_outputs = model(enc_inputs, dec_inputs)[decoding_step, 0, :]\n","        # 가장 큰 출력을 갖는 인덱스 얻어오기\n","        dec_output_idx = np.argmax(tensor2list(dec_outputs))\n","\n","        # 생성된 토큰은 dec_inputs에 추가 (첫번째 차원은 배치)\n","        dec_inputs[0][decoding_step+1] = dec_output_idx\n","\n","        # </S> 심볼 생성 시, Decoding 종료\n","        if idx2word[dec_output_idx] == \"</S>\":\n","            break\n","\n","        # 생성 토큰 추가\n","        response += idx2word[dec_output_idx]\n","    \n","    # <SP>를 공백으로 변환한 후 응답 문자열 출력\n","    print(response.replace(\"<SP>\", \" \"))\n","\n","def test(config):\n","\n","    # 어휘사전 읽어오기\n","    word2idx, idx2word = load_vocab(config['vocab_file'])\n","\n","    # Transformer Seq2Seq 모델 객체 생성\n","    model = TransformerChat(config).cuda()\n","\n","    # 학습한 모델 파일로부터 가중치 불러옴\n","    model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"trained_model_name\"])))\n","\n","    while(True):\n","        input_sequence = input(\"문장을 입력하세요. (종료는 exit을 입력하세요.) : \")\n","        if input_sequence == 'exit':\n","            break\n","        do_test(config, model, word2idx, idx2word, input_sequence)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ow01KJjz-416","executionInfo":{"status":"ok","timestamp":1638349525975,"user_tz":-540,"elapsed":7,"user":{"displayName":"우람이뭐하니","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcqGQ8DD1PE4ZzNLcy7NNEWePugigQphJ5VzuiNw=s64","userId":"17584337561729816007"}}},"source":["def train(config):\n","\n","    # Transformer Seq2Seq 모델 객체 생성\n","    model = TransformerChat(config).cuda()\n","\n","    # 데이터 읽기\n","    enc_inputs, dec_inputs, dec_outputs, word2idx, idx2word = load_dataset(config)\n","\n","    # TensorDataset/DataLoader를 통해 배치(batch) 단위로 데이터를 나누고 셔플(shuffle)\n","    train_features = TensorDataset(enc_inputs, dec_inputs, dec_outputs)\n","    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","    # 크로스엔트로피 손실 함수\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    # 옵티마이저 함수 지정\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learn_rate\"])\n","\n","    for epoch in range(config[\"epoch\"] + 1):\n","\n","        for (step, batch) in enumerate(train_dataloader):\n","\n","            # 학습 모드 셋팅\n","            model.train()\n","          \n","            # batch = (enc_inputs[step], dec_inputs[step], dec_outputs)*batch_size\n","            # .cuda()를 통해 메모리에 업로드\n","            batch = tuple(t.cuda() for t in batch)\n","            # 역전파 변화도 초기화\n","            optimizer.zero_grad()\n","\n","            enc_inputs, dec_inputs, dec_outputs = batch\n","            \n","            # hypothesis: [seq_len, batch, vocab_size] -> [seq_len*batch, vocab_size]\n","            hypothesis = model(enc_inputs, dec_inputs).view(-1, config['vocab_size'])\n","\n","            # labels: [batch, seq_len] -> [seq_len, batch] -> [seq_len(max_length)*batch]\n","            labels = dec_outputs.transpose(0, 1)\n","            labels = labels.reshape(config[\"max_length\"]*dec_inputs.size(0))\n","\n","            # 비용 계산 및 역전파 수행: cross_entopy 내부에서 labels를 원핫벡터로 변환 (골드레이블은 항상 1차원으로 입력)\n","            loss = loss_func(hypothesis, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 200 배치마다 중간 결과 출력\n","            if (step+1)% 200 == 0:\n","                print(\"Current Step : {0:d} / {1:d}\\tCurrent Loss : {2:f}\".format(step+1, int(len(enc_inputs) / config['batch_size']), loss.item()))\n","                # 생성 문장을 확인하기 위한 함수 호출\n","                # do_test(config, model, word2idx, idx2word)\n","\n","        # 에폭마다 가중치 저장\n","        torch.save(model.state_dict(), os.path.join(config[\"output_dir\"], \"epoch_{0:d}.pt\".format(epoch)))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fk1BLZ-VrHe","outputId":"ab030ea1-f5d3-4c83-b2cd-7f02899eafea"},"source":["\n","# colab gpu 할당 문제로 다른 구글 계정으로 학습하여 모델만 가져왔습니다.\n","\n","if(__name__==\"__main__\"):\n","\n","    root_dir = \"/gdrive/My Drive/Colab Notebooks/transformer/chatbot/\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    config = {\"mode\": \"train\",\n","              \"vocab_file\": os.path.join(root_dir, \"vocab.txt\"),\n","              \"train_file\": os.path.join(root_dir, \"train.txt\"),\n","              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n","              \"output_dir\":output_dir,\n","              \"epoch\": 10,\n","              \"learn_rate\":0.00005,\n","              \"num_encoder_layers\": 6,\n","              \"num_decoder_layers\": 6,\n","              \"num_heads\": 4,\n","              \"max_length\": 20,\n","              \"batch_size\": 128,\n","              \"embedding_size\": 256,\n","              \"hidden_size\": 512,\n","              \"vocab_size\": 4427\n","            }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        train(config)\n","    else:\n","        test(config)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 547958/547958 [00:11<00:00, 48117.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Current Step : 200 / 1\tCurrent Loss : 2.879473\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjJl2naFoVeI","executionInfo":{"status":"ok","timestamp":1638349709014,"user_tz":-540,"elapsed":110566,"user":{"displayName":"우람이뭐하니","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcqGQ8DD1PE4ZzNLcy7NNEWePugigQphJ5VzuiNw=s64","userId":"17584337561729816007"}},"outputId":"649393bc-de97-4c94-b9eb-8d76250a1d99"},"source":["if(__name__==\"__main__\"):\n","\n","    root_dir = \"/gdrive/My Drive/Colab Notebooks/transformer/chatbot/\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    config = {\"mode\": \"test\",\n","              \"vocab_file\": os.path.join(root_dir, \"vocab.txt\"),\n","              \"train_file\": os.path.join(root_dir, \"train.txt\"),\n","              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n","              \"output_dir\":output_dir,\n","              \"epoch\": 10,\n","              \"learn_rate\":0.00005,\n","              \"num_encoder_layers\": 6,\n","              \"num_decoder_layers\": 6,\n","              \"num_heads\": 4,\n","              \"max_length\": 20,\n","              \"batch_size\": 128,\n","              \"embedding_size\": 256,\n","              \"hidden_size\": 512,\n","              \"vocab_size\": 4427\n","            }\n","test(config)"],"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":["문장을 입력하세요. (종료는 exit을 입력하세요.) : 안녕하세요!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","  \n"]},{"name":"stdout","output_type":"stream","text":["안녕하세요\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 밥은 먹었어?\n","아니 안녕 먹었어\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 안녕!\n","안녕하세요\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 뭐해?\n","나 일하고 있어\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 영화 볼까?\n","아니 그냥 그냥 보내주고 싶은데\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 부대찌게 먹을까?\n","그럼 그럼 그럼 뭐 먹을까?\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 영화 보러 갈래요?\n","아 그래요?\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : exit\n"]}]}]}